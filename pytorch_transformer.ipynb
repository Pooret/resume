{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7hUDltBWzwbxin+TdssCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pooret/resume/blob/main/pytorch_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.arange(0, 50).unsqeeze(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "5TE7cV1lKGjs",
        "outputId": "8897f65e-5f62-447b-84ea-1b09f098ee75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'unsqeeze'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e18902c75e30>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqeeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqeeze'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**\n",
        "\n",
        "The purpose of these positional encodings is to inject some information about the relative or absolute position of the tokens in the sequence, since the transformer has no built-in notion of order.\n",
        "\n",
        "*pos* - position\n",
        "\n",
        "*i* - dimension\n",
        "\n",
        "Each dimenson of the positional encoding corresponds to a sinusoid and have wavelengths that form a geometric progression from $2\\pi$ to $10000 * 2\\pi$\n",
        "\n",
        "$${PE}_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{2i / d_{model}}})$$\n",
        "$${PE}_{(pos, 2i+1)} = \\cos(\\frac{pos}{10000^{2i / d_{model}}})$$\n",
        "\n"
      ],
      "metadata": {
        "id": "RP2cpRSwKswe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_f0w8gjJg4g",
        "outputId": "fafe184d-6c3d-4ea0-e590-1ad4c5b60fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "d_model = 512\n",
        "max_len = 100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=500):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.encoding = torch.zeros(1, max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float()* -(math.log(10000.0) / d_model)) # decreasing scaling factor length of d_model//2\n",
        "\n",
        "    self.encoding[:, :, 0::2] = torch.sin(position * div_term)\n",
        "    self.encoding[:, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    x - (batch_size, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    seq_len = x.size(1)\n",
        "    return x + self.encoding[:, :seq_len, :].to(device)\n",
        "\n",
        "pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "input_tensor = torch.zeros(1, max_len, d_model)\n",
        "output = pos_encoder(input_tensor)\n",
        "print(output.shape)  # (1, max_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathbf{Attention}(Q, K, V) = \\mathsf{softmax}(\\frac{QK^T}{\\sqrt{d_{k}}})V$$"
      ],
      "metadata": {
        "id": "rG4aWwbcc7nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "max_len = 100\n",
        "dropout = 0.1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self, d_model, dropout=0.1):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "    self.temperature = math.sqrt(d_model) # scaling factor\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    q, k, v (batch_size, n_heads, seq_len, d_k)\n",
        "    \"\"\"\n",
        "    # k transpose (batch_size, n_heads, d_k, seq_len)\n",
        "    attn = torch.matmul(q, k.transpose(-2, -1)) / self.temperature # attn - (batch_size, n_heads, seq_len, seq_len)\n",
        "    if mask is not None:\n",
        "      attn = attn.masked_fill(mask == 0, -1e9) # large negative numbers for softmax\n",
        "    attn = self.softmax(attn)\n",
        "    attn = self.dropout(attn)\n",
        "    output = torch.matmul(attn, v)\n",
        "    return output, attn # (batch_size, n_heads, seq_len, d_k) (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "attn = ScaledDotProductAttention(d_model, dropout)\n",
        "q = torch.rand(64, 10, d_model)  # (batch_size, seq_len, n_heads * d_k)\n",
        "k = torch.rand(64, 10, d_model)\n",
        "v = torch.rand(64, 10, d_model)\n",
        "output, attn_weights = attn(q, k, v)\n",
        "print(output.shape)  # (batch_size, seq_len, n_heads * d_k)\n",
        "print(attn_weights.shape)  # (batch_size, seq_len, seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx9Sp6uHPJ7b",
        "outputId": "e7ab74b6-9e1d-4daf-a3a2-25637c3fff42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n",
            "torch.Size([64, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, dropout = 0.1):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.d_k = d_model // n_heads\n",
        "\n",
        "    self.q_linear = nn.Linear(d_model, d_model)\n",
        "    self.k_linear = nn.Linear(d_model, d_model)\n",
        "    self.v_linear = nn.Linear(d_model, d_model)\n",
        "    self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.attention = ScaledDotProductAttention(self.d_k, dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layer_norm = nn.LayerNorm(d_model) # normalize across features dimension\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    batch_size = q.size(0) # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # q, k, v - (batch_size, seq_len, n_heads, d_k)\n",
        "    q = self.q_linear(q).view(batch_size, -1, self.n_heads, self.d_k) # n_heads * d_k = d_model\n",
        "    k = self.k_linear(k).view(batch_size, -1, self.n_heads, self.d_k)\n",
        "    v = self.v_linear(v).view(batch_size, -1, self.n_heads, self.d_k)\n",
        "\n",
        "    # (batch_size, seq_len, n_heads, d_k) -> (batch_size, n_heads, seq_len, d_k)\n",
        "    q = q.transpose(1,2)\n",
        "    k = k.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "\n",
        "    attn_output, attn_weights = self.attention(q, k, v, mask) # outputs - (batch_size, n_heads, seq_len, d_k) (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "    # The contiguous() function is required because transpose may change the memory layout, making it non-contiguous. The view function requires a contiguous tensor\n",
        "    attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model) #  (batch_size, n_heads, seq_len, d_k) - > (batch_size, seq_len, n_heads * d_k)\n",
        "    output = self.dropout(self.fc(attn_output)) # (batch_size, seq_len, d_model)\n",
        "    output = self.layer_norm(output + q.reshape(batch_size, -1, self.d_model))\n",
        "\n",
        "    return output, attn_weights # (batch_size, seq_len, d_model), (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "dropout = 0.1\n",
        "multi_head_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "q = torch.rand(64, 10, d_model)  # (batch_size, seq_len, n_heads * d_k)\n",
        "k = torch.rand(64, 10, d_model)\n",
        "v = torch.rand(64, 10, d_model)\n",
        "output, attn_weights = multi_head_attn(q, k, v)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)\n",
        "print(attn_weights.shape)  #(batch_size, n_heads, seq_len, seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b94g15TuTbhA",
        "outputId": "a33f4fda-a787-42bc-cf73-b22f1f47fc68"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n",
            "torch.Size([64, 8, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm(x + residual) # normalize across features dimension\n",
        "    return x\n",
        "\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "input_tensor = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "output = ffn(input_tensor)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1-Hgv1MbWXy",
        "outputId": "2d28c9ab-636f-4647-acb4-97ede9ce40d4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention sublayer\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = self.layer_norm(x + attn_output)  # Add & Norm\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        x = self.feed_forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 1\n",
        "dropout = 0.1\n",
        "encoder_layer = EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "input_tensor = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "output = encoder_layer(input_tensor)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mQ9naFDiHKY",
        "outputId": "5cde9793-28d5-44e8-fe97-95890d578a11"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "    # self-attention sublayer\n",
        "    self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
        "    x = self.layer_norm(x + self_attn_output) # add and norm\n",
        "\n",
        "    # cross-attention sublayer (q->x, k,v -> enc_output)\n",
        "    cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "\n",
        "    # feed-forward sublayer\n",
        "    x = self.feed_forward(x)\n",
        "\n",
        "    return x # (batch_size, seq_len, d_model)\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "decoder_layer = DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "input_tensor = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "enc_output = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "output = decoder_layer(input_tensor, enc_output)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ8OhygojNme",
        "outputId": "5a1ac295-3606-4275-ae6f-4bdcfd187b5f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, src_vocab_size, tgt_vocab_size, max_len, dropout=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "    self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "    self.encoder = Encoder(d_model, n_heads, d_ff, num_layers, dropout)\n",
        "    self.decoder = Decoder(d_model, n_heads, d_ff, num_layers, dropout)\n",
        "    self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "    # Encoder\n",
        "    enc_output = self.encoder_embedding(src) # enc_output - (batch_size, src_len, d_model)\n",
        "    enc_output = self.positional_encoding(enc_output)\n",
        "    enc_output = self.encoder(enc_output, src_mask)\n",
        "\n",
        "    # Decoder\n",
        "    dec_output = self.decoder_embedding(tgt) # dec_output - (batch_size, tgt_len, d_model)\n",
        "    dec_output = self.positional_encoding(enc_output)\n",
        "    dec_output = self.decoder(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "    # Final layer\n",
        "    output = self.fc(dec_output) # (batch_size, tgt_len, tgt_vocab_size)\n",
        "    return output\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "src_vocab_size = 10000\n",
        "tgt_vocab_size = 10000\n",
        "max_len = 100\n",
        "dropout = 0.1\n",
        "transformer = Transformer(d_model, n_heads, d_ff, num_layers, src_vocab_size, tgt_vocab_size, max_len, dropout)\n",
        "src = torch.randint(0, src_vocab_size, (64, 10))  # (batch_size, src_len)\n",
        "tgt = torch.randint(0, tgt_vocab_size, (64, 10))  # (batch_size, tgt_len)\n",
        "output = transformer(src, tgt)\n",
        "print(output.shape) # (batch_size, tgt_len, tgt_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLgRZWSOntRb",
        "outputId": "a40bae4e-a3d8-40b8-8f64-0d568b55e5d7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBys3VuYseU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}