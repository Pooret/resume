{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSaCgMNwVDSBMbvtZa0GZb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pooret/resume/blob/main/pytorch_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-GyZvHvwfMQ",
        "outputId": "151cfe39-2027-4c42-c974-7f87aea96cc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**\n",
        "\n",
        "The purpose of these positional encodings is to inject some information about the relative or absolute position of the tokens in the sequence, since the transformer has no built-in notion of order.\n",
        "\n",
        "*pos* - position\n",
        "\n",
        "*i* - dimension\n",
        "\n",
        "Each dimenson of the positional encoding corresponds to a sinusoid and have wavelengths that form a geometric progression from $2\\pi$ to $10000 * 2\\pi$\n",
        "\n",
        "$${PE}_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{2i / d_{model}}})$$\n",
        "$${PE}_{(pos, 2i+1)} = \\cos(\\frac{pos}{10000^{2i / d_{model}}})$$\n",
        "\n"
      ],
      "metadata": {
        "id": "RP2cpRSwKswe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_f0w8gjJg4g",
        "outputId": "30c757aa-a81e-41c3-bef5-a751fa65bc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "d_model = 512\n",
        "max_len = 100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=500):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.encoding = torch.zeros(1, max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float()* -(math.log(10000.0) / d_model)) # decreasing scaling factor length of d_model//2\n",
        "\n",
        "    self.encoding[:, :, 0::2] = torch.sin(position * div_term)\n",
        "    self.encoding[:, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    x - (batch_size, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    seq_len = x.size(1)\n",
        "    return x + self.encoding[:, :seq_len, :].to(device)\n",
        "\n",
        "pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "input_tensor = torch.zeros(1, max_len, d_model).to(device)\n",
        "output = pos_encoder(input_tensor)\n",
        "print(output.shape)  # (1, max_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathbf{Attention}(Q, K, V) = \\mathsf{softmax}(\\frac{QK^T}{\\sqrt{d_{k}}})V$$"
      ],
      "metadata": {
        "id": "rG4aWwbcc7nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "max_len = 100\n",
        "dropout = 0.1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self, d_model, dropout=0.1):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "    self.temperature = math.sqrt(d_model) # scaling factor\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    q, k, v (batch_size, n_heads, seq_len, d_k)\n",
        "    \"\"\"\n",
        "    # k transpose (batch_size, n_heads, d_k, seq_len)\n",
        "    attn = torch.matmul(q, k.transpose(-2, -1)) / self.temperature # attn - (batch_size, n_heads, seq_len, seq_len)\n",
        "    if mask is not None:\n",
        "      attn = attn.masked_fill(mask == 0, -1e9) # large negative numbers for softmax\n",
        "    attn = self.softmax(attn)\n",
        "    attn = self.dropout(attn)\n",
        "    output = torch.matmul(attn, v)\n",
        "    return output, attn # (batch_size, n_heads, seq_len, d_k) (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "attn = ScaledDotProductAttention(d_model, dropout)\n",
        "q = torch.rand(64, 10, d_model)  # (batch_size, seq_len, n_heads * d_k)\n",
        "k = torch.rand(64, 10, d_model)\n",
        "v = torch.rand(64, 10, d_model)\n",
        "output, attn_weights = attn(q, k, v)\n",
        "print(output.shape)  # (batch_size, seq_len, n_heads * d_k)\n",
        "print(attn_weights.shape)  # (batch_size, seq_len, seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx9Sp6uHPJ7b",
        "outputId": "38cfc47c-8eb7-49fa-8a83-e2779da9143b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n",
            "torch.Size([64, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, dropout = 0.1):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.d_k = d_model // n_heads\n",
        "\n",
        "    self.q_linear = nn.Linear(d_model, d_model)\n",
        "    self.k_linear = nn.Linear(d_model, d_model)\n",
        "    self.v_linear = nn.Linear(d_model, d_model)\n",
        "    self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.attention = ScaledDotProductAttention(self.d_k, dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layer_norm = nn.LayerNorm(d_model) # normalize across features dimension\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    batch_size = q.size(0) # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # q, k, v - (batch_size, seq_len, n_heads, d_k)\n",
        "    q = self.q_linear(q).view(batch_size, -1, self.n_heads, self.d_k) # n_heads * d_k = d_model\n",
        "    k = self.k_linear(k).view(batch_size, -1, self.n_heads, self.d_k)\n",
        "    v = self.v_linear(v).view(batch_size, -1, self.n_heads, self.d_k)\n",
        "\n",
        "    # (batch_size, seq_len, n_heads, d_k) -> (batch_size, n_heads, seq_len, d_k)\n",
        "    q = q.transpose(1,2)\n",
        "    k = k.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "\n",
        "    attn_output, attn_weights = self.attention(q, k, v, mask) # outputs - (batch_size, n_heads, seq_len, d_k) (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "    # The contiguous() function is required because transpose may change the memory layout, making it non-contiguous. The view function requires a contiguous tensor\n",
        "    attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model) #  (batch_size, n_heads, seq_len, d_k) - > (batch_size, seq_len, n_heads * d_k)\n",
        "    output = self.dropout(self.fc(attn_output)) # (batch_size, seq_len, d_model)\n",
        "    output = self.layer_norm(output + q.reshape(batch_size, -1, self.d_model))\n",
        "\n",
        "    return output, attn_weights # (batch_size, seq_len, d_model), (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "dropout = 0.1\n",
        "multi_head_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "q = torch.rand(64, 10, d_model)  # (batch_size, seq_len, n_heads * d_k)\n",
        "k = torch.rand(64, 10, d_model)\n",
        "v = torch.rand(64, 10, d_model)\n",
        "output, attn_weights = multi_head_attn(q, k, v)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)\n",
        "print(attn_weights.shape)  #(batch_size, n_heads, seq_len, seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b94g15TuTbhA",
        "outputId": "07b50ca3-f5c1-424e-f73f-5c64628610e6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n",
            "torch.Size([64, 8, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm(x + residual) # normalize across features dimension\n",
        "    return x\n",
        "\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "input_tensor = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "output = ffn(input_tensor)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1-Hgv1MbWXy",
        "outputId": "2cfaf448-afa8-46ee-9480-7c48af908fc2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention sublayer\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = self.layer_norm(x + attn_output)  # Add & Norm\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        x = self.feed_forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 1\n",
        "dropout = 0.1\n",
        "encoder_layer = EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "input_tensor = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "output = encoder_layer(input_tensor)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mQ9naFDiHKY",
        "outputId": "67f672c6-4448-448f-9bc5-b2c2cb0113c0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "    # self-attention sublayer\n",
        "    self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
        "    x = self.layer_norm(x + self_attn_output) # add and norm\n",
        "\n",
        "    # cross-attention sublayer (q->x, k,v -> enc_output)\n",
        "    cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "\n",
        "    # feed-forward sublayer\n",
        "    x = self.feed_forward(x)\n",
        "\n",
        "    return x # (batch_size, seq_len, d_model)\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "decoder_layer = DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "input_tensor = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "enc_output = torch.rand(64, 10, d_model)  # (batch_size, seq_len, d_model)\n",
        "output = decoder_layer(input_tensor, enc_output)\n",
        "print(output.shape)  # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ8OhygojNme",
        "outputId": "b526d09c-df37-4b67-c5c7-5b966c8c79e5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, src_vocab_size, tgt_vocab_size, max_len, dropout=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "    self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "    self.encoder = Encoder(d_model, n_heads, d_ff, num_layers, dropout)\n",
        "    self.decoder = Decoder(d_model, n_heads, d_ff, num_layers, dropout)\n",
        "    self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "    # Encoder\n",
        "    enc_output = self.encoder_embedding(src) # enc_output - (batch_size, src_len, d_model)\n",
        "    enc_output = self.positional_encoding(enc_output)\n",
        "    enc_output = self.encoder(enc_output, src_mask)\n",
        "\n",
        "    # Decoder\n",
        "    dec_output = self.decoder_embedding(tgt) # dec_output - (batch_size, tgt_len, d_model)\n",
        "    dec_output = self.positional_encoding(enc_output)\n",
        "    dec_output = self.decoder(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "    # Final layer\n",
        "    output = self.fc(dec_output) # (batch_size, tgt_len, tgt_vocab_size)\n",
        "    return output\n",
        "\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "src_vocab_size = 10000\n",
        "tgt_vocab_size = 10000\n",
        "max_len = 100\n",
        "dropout = 0.1\n",
        "transformer = Transformer(d_model, n_heads, d_ff, num_layers, src_vocab_size, tgt_vocab_size, max_len, dropout).to(device)\n",
        "src = torch.randint(0, src_vocab_size, (64, 10)).to(device)  # (batch_size, src_len)\n",
        "tgt = torch.randint(0, tgt_vocab_size, (64, 10)).to(device)  # (batch_size, tgt_len)\n",
        "output = transformer(src, tgt)\n",
        "print(output.shape) # (batch_size, tgt_len, tgt_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLgRZWSOntRb",
        "outputId": "3052fc18-69d8-4efe-a7ac-db9f32e3cdfd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets/chemdata/smiles_to_iupac/df_small.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "WBys3VuYseU1",
        "outputId": "0aab7654-222d-4d1f-a46e-5ba0daa753d6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   molregno     canonical_smiles  activity_id standard_type  standard_value  \\\n",
              "0       188   COC(=O)[C@@H](N)CO       339708            Km     34000000.00   \n",
              "1       238   CCc1cc2c(O)ncnc2s1     16421311       Potency       100000.00   \n",
              "2       242  C1=C(c2ccccc2)CCNC1       757035          IC50     12000000.00   \n",
              "3       265  CCc1cc2c(Cl)ncnc2s1       994954          IC50       100000.00   \n",
              "4       423   CSc1ccc(CC(C)N)cc1      5232140            Ki       630957.34   \n",
              "\n",
              "  standard_units  assay_id     tid target_chembl_id           organism  ...  \\\n",
              "0             nM     68870   11770       CHEMBL2943  Rattus norvegicus  ...   \n",
              "1             nM   1543582  114868     CHEMBL612558       Homo sapiens  ...   \n",
              "2             nM    226540   13054       CHEMBL3998  Rattus norvegicus  ...   \n",
              "3             nM    213817   22224     CHEMBL612558       Homo sapiens  ...   \n",
              "4             nM    725353  106368     CHEMBL614910  Rattus norvegicus  ...   \n",
              "\n",
              "  hba_lipinski  hbd_lipinski  num_lipinski_ro5_violations  np_likeness_score  \\\n",
              "0          4.0           3.0                          0.0               0.91   \n",
              "1          3.0           1.0                          0.0              -1.96   \n",
              "2          1.0           1.0                          0.0               0.27   \n",
              "3          2.0           0.0                          0.0              -2.23   \n",
              "4          1.0           2.0                          0.0              -0.66   \n",
              "\n",
              "     bei    le   lle    sei  smiles_len  \\\n",
              "0    NaN   NaN   NaN    NaN          18   \n",
              "1  27.02  0.55  2.91  10.58          18   \n",
              "2  37.68  0.68  3.94  49.88          19   \n",
              "3  23.88  0.54  1.83  18.40          19   \n",
              "4  39.33  0.81  4.83  27.40          18   \n",
              "\n",
              "                                      iupac  \n",
              "0   methyl (2S)-2-amino-3-hydroxypropanoate  \n",
              "1   6-ethyl-3H-thieno[2,3-d]pyrimidin-4-one  \n",
              "2       4-phenyl-1,2,3,6-tetrahydropyridine  \n",
              "3   4-chloro-6-ethylthieno[2,3-d]pyrimidine  \n",
              "4  1-(4-methylsulfanylphenyl)propan-2-amine  \n",
              "\n",
              "[5 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b661052b-88b3-4239-8ea3-5a4000854b39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>molregno</th>\n",
              "      <th>canonical_smiles</th>\n",
              "      <th>activity_id</th>\n",
              "      <th>standard_type</th>\n",
              "      <th>standard_value</th>\n",
              "      <th>standard_units</th>\n",
              "      <th>assay_id</th>\n",
              "      <th>tid</th>\n",
              "      <th>target_chembl_id</th>\n",
              "      <th>organism</th>\n",
              "      <th>...</th>\n",
              "      <th>hba_lipinski</th>\n",
              "      <th>hbd_lipinski</th>\n",
              "      <th>num_lipinski_ro5_violations</th>\n",
              "      <th>np_likeness_score</th>\n",
              "      <th>bei</th>\n",
              "      <th>le</th>\n",
              "      <th>lle</th>\n",
              "      <th>sei</th>\n",
              "      <th>smiles_len</th>\n",
              "      <th>iupac</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>188</td>\n",
              "      <td>COC(=O)[C@@H](N)CO</td>\n",
              "      <td>339708</td>\n",
              "      <td>Km</td>\n",
              "      <td>34000000.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>68870</td>\n",
              "      <td>11770</td>\n",
              "      <td>CHEMBL2943</td>\n",
              "      <td>Rattus norvegicus</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.91</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18</td>\n",
              "      <td>methyl (2S)-2-amino-3-hydroxypropanoate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>238</td>\n",
              "      <td>CCc1cc2c(O)ncnc2s1</td>\n",
              "      <td>16421311</td>\n",
              "      <td>Potency</td>\n",
              "      <td>100000.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>1543582</td>\n",
              "      <td>114868</td>\n",
              "      <td>CHEMBL612558</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>27.02</td>\n",
              "      <td>0.55</td>\n",
              "      <td>2.91</td>\n",
              "      <td>10.58</td>\n",
              "      <td>18</td>\n",
              "      <td>6-ethyl-3H-thieno[2,3-d]pyrimidin-4-one</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>242</td>\n",
              "      <td>C1=C(c2ccccc2)CCNC1</td>\n",
              "      <td>757035</td>\n",
              "      <td>IC50</td>\n",
              "      <td>12000000.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>226540</td>\n",
              "      <td>13054</td>\n",
              "      <td>CHEMBL3998</td>\n",
              "      <td>Rattus norvegicus</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>37.68</td>\n",
              "      <td>0.68</td>\n",
              "      <td>3.94</td>\n",
              "      <td>49.88</td>\n",
              "      <td>19</td>\n",
              "      <td>4-phenyl-1,2,3,6-tetrahydropyridine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>265</td>\n",
              "      <td>CCc1cc2c(Cl)ncnc2s1</td>\n",
              "      <td>994954</td>\n",
              "      <td>IC50</td>\n",
              "      <td>100000.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>213817</td>\n",
              "      <td>22224</td>\n",
              "      <td>CHEMBL612558</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.23</td>\n",
              "      <td>23.88</td>\n",
              "      <td>0.54</td>\n",
              "      <td>1.83</td>\n",
              "      <td>18.40</td>\n",
              "      <td>19</td>\n",
              "      <td>4-chloro-6-ethylthieno[2,3-d]pyrimidine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>423</td>\n",
              "      <td>CSc1ccc(CC(C)N)cc1</td>\n",
              "      <td>5232140</td>\n",
              "      <td>Ki</td>\n",
              "      <td>630957.34</td>\n",
              "      <td>nM</td>\n",
              "      <td>725353</td>\n",
              "      <td>106368</td>\n",
              "      <td>CHEMBL614910</td>\n",
              "      <td>Rattus norvegicus</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>39.33</td>\n",
              "      <td>0.81</td>\n",
              "      <td>4.83</td>\n",
              "      <td>27.40</td>\n",
              "      <td>18</td>\n",
              "      <td>1-(4-methylsulfanylphenyl)propan-2-amine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 40 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b661052b-88b3-4239-8ea3-5a4000854b39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b661052b-88b3-4239-8ea3-5a4000854b39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b661052b-88b3-4239-8ea3-5a4000854b39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f743946e-1d73-4086-a11e-753de1d56c5c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f743946e-1d73-4086-a11e-753de1d56c5c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f743946e-1d73-4086-a11e-753de1d56c5c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df['iupac'].isnull()].reset_index(drop=True)\n",
        "smiles = df['canonical_smiles']\n",
        "labels = df['iupac']\n",
        "smiles, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z15yuezu7Jm",
        "outputId": "c3c7e655-2a84-464e-8773-1ee37f391808"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0        COC(=O)[C@@H](N)CO\n",
              " 1        CCc1cc2c(O)ncnc2s1\n",
              " 2       C1=C(c2ccccc2)CCNC1\n",
              " 3       CCc1cc2c(Cl)ncnc2s1\n",
              " 4        CSc1ccc(CC(C)N)cc1\n",
              "                ...         \n",
              " 7364      Cc1csc(N)c1C(N)=O\n",
              " 7365      Cc1cc(CO)c2n1CSC2\n",
              " 7366      C#CC(=O)Nc1ccccc1\n",
              " 7367          C#CCNC(=O)C#C\n",
              " 7368           N#Cc1cccnc1O\n",
              " Name: canonical_smiles, Length: 7369, dtype: object,\n",
              " 0                 methyl (2S)-2-amino-3-hydroxypropanoate\n",
              " 1                 6-ethyl-3H-thieno[2,3-d]pyrimidin-4-one\n",
              " 2                     4-phenyl-1,2,3,6-tetrahydropyridine\n",
              " 3                 4-chloro-6-ethylthieno[2,3-d]pyrimidine\n",
              " 4                1-(4-methylsulfanylphenyl)propan-2-amine\n",
              "                               ...                        \n",
              " 7364              2-amino-4-methylthiophene-3-carboxamide\n",
              " 7365    (5-methyl-1,3-dihydropyrrolo[1,2-c][1,3]thiazo...\n",
              " 7366                               N-phenylprop-2-ynamide\n",
              " 7367                          N-prop-2-ynylprop-2-ynamide\n",
              " 7368                     2-oxo-1H-pyridine-3-carbonitrile\n",
              " Name: iupac, Length: 7369, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "# Tokenization functions\n",
        "def tokenize_iupac(name):\n",
        "    patterns = [\n",
        "        r'\\([^\\)]+\\)',  # Match groups inside parentheses\n",
        "        r'\\[[^\\]]+\\]',  # Match groups inside square brackets\n",
        "        r'[A-Za-z]+-\\d+',  # Match chemical groups with numbers (e.g., ethyl-6)\n",
        "        r'\\d+-[A-Za-z]+',  # Match numbers with chemical groups (e.g., 2-amino)\n",
        "        r'\\d+',  # Match standalone numbers\n",
        "        r'[A-Za-z]+',  # Match standalone chemical groups\n",
        "        r'[^\\s\\w]',     # Match remaining single non-word characters\n",
        "    ]\n",
        "    combined_pattern = '|'.join(patterns)\n",
        "    tokens = re.findall(combined_pattern, name)\n",
        "    return tokens\n",
        "\n",
        "def tokenize_smiles(smiles):\n",
        "    # Simple tokenization for SMILES (you can replace with a more complex tokenizer if needed)\n",
        "    return list(smiles)\n",
        "\n",
        "\n",
        "# Tokenize the SMILES and IUPAC names\n",
        "df['smiles_tokens'] = df['canonical_smiles'].apply(tokenize_smiles)\n",
        "df['iupac_tokens'] = df['iupac'].apply(tokenize_iupac)\n",
        "\n",
        "# Build the vocabularies\n",
        "def build_vocab(token_list, min_freq=1):\n",
        "    counter = Counter(itertools.chain.from_iterable(token_list))\n",
        "    vocab = {token: idx for idx, (token, freq) in enumerate(counter.items(), start=4) if freq >= min_freq}\n",
        "    vocab['<pad>'] = 0\n",
        "    vocab['<sos>'] = 1\n",
        "    vocab['<eos>'] = 2\n",
        "    vocab['<unk>'] = 3\n",
        "    return vocab\n",
        "\n",
        "smiles_vocab = build_vocab(df['smiles_tokens'])\n",
        "iupac_vocab = build_vocab(df['iupac_tokens'])\n",
        "\n",
        "# Convert sentences to sequences of token IDs\n",
        "def sentence_to_ids(sentence, vocab, tokenize_fn, add_special_tokens=True):\n",
        "    tokens = tokenize_fn(sentence)\n",
        "    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "    if add_special_tokens:\n",
        "        ids = [vocab['<sos>']] + ids + [vocab['<eos>']]\n",
        "    return ids\n",
        "\n",
        "df['smiles_ids'] = df['canonical_smiles'].apply(lambda x: sentence_to_ids(x, smiles_vocab, tokenize_smiles))\n",
        "df['iupac_ids'] = df['iupac'].apply(lambda x: sentence_to_ids(x, iupac_vocab, tokenize_iupac))\n",
        "\n",
        "# Define the Dataset class\n",
        "class ChemistryDataset(Dataset):\n",
        "    def __init__(self, df, src_vocab, tgt_vocab, max_len=100):\n",
        "        self.df = df\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_seq = self.df.iloc[idx]['smiles_ids']\n",
        "        tgt_seq = self.df.iloc[idx]['iupac_ids']\n",
        "        src_seq = self.pad_sequence(src_seq, self.src_vocab)\n",
        "        tgt_seq = self.pad_sequence(tgt_seq, self.tgt_vocab)\n",
        "        return torch.tensor(src_seq), torch.tensor(tgt_seq)\n",
        "\n",
        "    def pad_sequence(self, seq, vocab):\n",
        "        if len(seq) < self.max_len:\n",
        "            seq = seq + [vocab['<pad>']] * (self.max_len - len(seq))\n",
        "        else:\n",
        "            seq = seq[:self.max_len]\n",
        "        return seq\n",
        "\n",
        "# Initialize Dataset and DataLoader\n",
        "max_len = 50\n",
        "dataset = ChemistryDataset(df, smiles_vocab, iupac_vocab, max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Example: Check a batch\n",
        "for src_batch, tgt_batch in dataloader:\n",
        "    print(src_batch)\n",
        "    print(tgt_batch)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "187vvSCTIa9-",
        "outputId": "e403cd61-4b3e-4504-d68c-ac6ea6ee8362"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 13, 13,  ...,  0,  0,  0],\n",
            "        [ 1,  5,  7,  ...,  0,  0,  0],\n",
            "        [ 1,  4,  4,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [ 1,  4,  5,  ...,  0,  0,  0],\n",
            "        [ 1, 13, 26,  ...,  0,  0,  0],\n",
            "        [ 1, 20, 14,  ...,  0,  0,  0]])\n",
            "tensor([[   1, 2574,    6,  ...,    0,    0,    0],\n",
            "        [   1, 3221,    6,  ...,    0,    0,    0],\n",
            "        [   1,  413,    6,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  176,    6,  ...,    0,    0,    0],\n",
            "        [   1,  257,    6,  ...,    0,    0,    0],\n",
            "        [   1, 2387,    6,  ...,    0,    0,    0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "src_vocab_size = len(smiles_vocab)\n",
        "tgt_vocab_size = len(iupac_vocab)\n",
        "max_len = 50\n",
        "dropout = 0.1\n",
        "num_epochs = 20\n",
        "learning_rate = 1e-4\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model, n_heads, d_ff, num_layers, src_vocab_size, tgt_vocab_size, max_len, dropout).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=smiles_vocab['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with tqdm\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for src_batch, tgt_batch in progress_bar:\n",
        "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src_batch, tgt_batch[:, :-1])\n",
        "\n",
        "        # Adjust reshaping to match the target sequence length\n",
        "        output = output[:, :-1, :].contiguous().view(-1, output.size(-1))\n",
        "        tgt_batch = tgt_batch[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=epoch_loss/len(dataloader))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-RZcbwJCA8r",
        "outputId": "8319db31-f54e-40c0-c706-d61224e1ddab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.27it/s, loss=4.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 4.616379322943749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.41it/s, loss=3.97]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Loss: 3.969528725652984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.39it/s, loss=3.47]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Loss: 3.4718978848808257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.37it/s, loss=3.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Loss: 3.0190702609685593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.41it/s, loss=2.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Loss: 2.583978627151225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.40it/s, loss=2.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Loss: 2.178943733116249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.39it/s, loss=1.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Loss: 1.7854753820411056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.41it/s, loss=1.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Loss: 1.436268154160801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.40it/s, loss=1.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Loss: 1.111081700046341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.40it/s, loss=0.836]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Loss: 0.8362588332845019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.39it/s, loss=0.609]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Loss: 0.6085210788301575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.40it/s, loss=0.448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Loss: 0.44843552664760905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.39it/s, loss=0.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Loss: 0.3295167518771572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.39it/s, loss=0.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20, Loss: 0.25008180273043645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.41it/s, loss=0.195]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20, Loss: 0.19481092262448688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.37it/s, loss=0.164]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20, Loss: 0.16387022038300833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.40it/s, loss=0.131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20, Loss: 0.13113447562569663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.41it/s, loss=0.112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20, Loss: 0.11218131556139364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.40it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20, Loss: 0.1131838307881252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:36<00:00,  6.39it/s, loss=0.0933]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20, Loss: 0.09328456190757421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_smiles(model, smiles, src_vocab, tgt_vocab, max_len):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input SMILES string\n",
        "    src_seq = sentence_to_ids(smiles, src_vocab, tokenize_smiles, add_special_tokens=False)\n",
        "    src_seq = torch.tensor(src_seq).unsqueeze(0).to(device)\n",
        "\n",
        "    # Prepare the input tensor with padding if necessary\n",
        "    if src_seq.size(1) < max_len:\n",
        "        src_seq = torch.cat([src_seq, torch.tensor([src_vocab['<pad>']] * (max_len - src_seq.size(1))).unsqueeze(0).to(device)], dim=1)\n",
        "    else:\n",
        "        src_seq = src_seq[:, :max_len]\n",
        "\n",
        "    # Prepare the source mask\n",
        "    src_mask = (src_seq != src_vocab['<pad>']).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # Pass through the encoder\n",
        "    with torch.no_grad():\n",
        "        memory = model.encoder_embedding(src_seq)\n",
        "        memory = model.positional_encoding(memory)\n",
        "        memory = model.encoder(memory, src_mask)\n",
        "\n",
        "    # Prepare the initial decoder input\n",
        "    tgt_seq = torch.tensor([tgt_vocab['<sos>']]).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Prepare the target mask\n",
        "        tgt_mask = (tgt_seq != tgt_vocab['<pad>']).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Pass through the decoder\n",
        "        with torch.no_grad():\n",
        "            tgt_emb = model.decoder_embedding(tgt_seq)\n",
        "            tgt_emb = model.positional_encoding(tgt_emb)\n",
        "            tgt_emb = model.decoder(tgt_emb, memory, src_mask, tgt_mask)\n",
        "            output = model.fc(tgt_emb)\n",
        "\n",
        "        # Get the next token\n",
        "        next_token = output.argmax(dim=-1)[:, -1]\n",
        "        generated_tokens.append(next_token.item())\n",
        "\n",
        "        # Update the target sequence\n",
        "        tgt_seq = torch.cat([tgt_seq, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        # Stop if the end of sequence token is generated\n",
        "        if next_token.item() == tgt_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Convert token IDs to tokens\n",
        "    translated_tokens = [list(tgt_vocab.keys())[list(tgt_vocab.values()).index(token)] for token in generated_tokens]\n",
        "\n",
        "    return ' '.join(translated_tokens)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming the model and vocabularies are properly defined and trained\n",
        "translated_iupac = translate_smiles(model, df['canonical_smiles'].iloc[1], smiles_vocab, iupac_vocab, max_len)\n",
        "print(f\"Translated IUPAC name: {translated_iupac}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ug4R3e5Bslk",
        "outputId": "220126d3-cc59-4ff7-98de-c67c4d2beac5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated IUPAC name: <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhWAcaR3JNRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}